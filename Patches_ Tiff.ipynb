{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import *\n",
    "import tifffile as tiff\n",
    "import imageio as io\n",
    "import cv2\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction import image\n",
    "from sklearn.preprocessing import MinMaxScaler as mm\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for ima in os.listdir('/home/salijona/Documents/UT/NeuralNetworksProject/Noisy_Images/Noisy_Images/'):\n",
    "    i=i+1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_full=np.ndarray(shape=(840,1080,1080))\n",
    "#noisy_full=np.ndarray(shape=(840,1080,1080))\n",
    "\n",
    "scaler=mm(feature_range=(0, 1))\n",
    "img_name=[]\n",
    "i=0\n",
    "for ima in os.listdir('/home/salijona/Documents/UT/NeuralNetworksProject/TIFF/TIFF_Original/Images'):\n",
    "    img_in = np.array(cv2.imread('/home/salijona/Documents/UT/NeuralNetworksProject/TIFF/TIFF_Original/Images/'+ima,cv2.IMREAD_ANYDEPTH), dtype=np.uint16)\n",
    "    img_in=img_in.clip(0)\n",
    "   \n",
    "    plt.figure(figsize=(20,10))\n",
    "\n",
    "    plt.imshow(img_in, cmap=\"gray\")\n",
    "    plt.show()\n",
    "   # scaler.fit(img_in)\n",
    "    #t = scaler.transform(img_in)\n",
    "    #img_in =t.reshape(img_in.shape)\n",
    "\n",
    "    rmin=np.min(img_in)\n",
    "    rmax=np.max(img_in)\n",
    "    img_in=(img_in-rmin)/(rmax-rmin)\n",
    "    #img_in = int(255 * (img_in - rmin) / (rmax - rmin) )\n",
    "    img_in=img_in.clip(0)\n",
    "    img_in=np.clip(img_in,a_min=0,a_max=1)\n",
    "    plt.figure(figsize=(20,10))\n",
    "\n",
    "    plt.imshow(img_in, cmap=\"gray\")\n",
    "    plt.show()\n",
    "    original_full[i]=img_in\n",
    "    print(np.min(img_in))\n",
    "    print(np.max(img_in))\n",
    "\n",
    "    \n",
    "    img_in = np.array(cv2.imread('/home/salijona/Documents/UT/NeuralNetworksProject/Noisy_Images/Noisy_Images/'+ima,cv2.IMREAD_ANYDEPTH), dtype=np.int16)\n",
    "    img_in=img_in.clip(0)\n",
    "\n",
    "    rmin=np.min(img_in)\n",
    "    rmax=np.max(img_in)\n",
    "    img_in=(img_in-rmin)/(rmax-rmin)\n",
    "    #img_in = int(255 * (img_in - rmin) / (rmax - rmin) )\n",
    "    img_in=img_in.clip(0)\n",
    "    img_in=np.clip(img_in,a_min=0,a_max=1)\n",
    "    plt.figure(figsize=(20,10))\n",
    "\n",
    "    plt.imshow(img_in, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "    noisy_full[i]=img_in\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    i=i+1\n",
    "\n",
    "with h5py.File('original_full.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"original_full\",  data=original_full)\n",
    "\n",
    "with h5py.File('noisy_full.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"noisy_full\",  data=noisy_full)\n",
    "\n",
    "#np.save('original_full',original_full)\n",
    "#np.save('noisy_full',noisy_full)\n",
    "\n",
    "del original_full\n",
    "del noisy_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('noisy_full.h5', 'r')\n",
    "all_data = np.array(f.get('noisy_full'))[:, :, :]\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "n_inst = all_data.shape[0] #total number of images\n",
    "np.random.seed(22)\n",
    "indices = np.random.permutation(n_inst) #\n",
    "n_train = int(0.8 * n_inst)\n",
    "training_idx, test_idx = indices[:n_train], indices[n_train:]\n",
    "\n",
    "data, test_data  = all_data[training_idx,:], all_data[test_idx,:]\n",
    "\n",
    "\n",
    "with h5py.File('data.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"data\",  data=data)    \n",
    "    \n",
    "with h5py.File('test_data.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"test_data\",  data=test_data)  \n",
    "\n",
    "    \n",
    "    \n",
    "del all_data\n",
    "del data\n",
    "del test_data\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = h5py.File('original_full.h5', 'r')\n",
    "all_label = np.array(f.get('original_full'))[:, :, :]\n",
    "f.close()\n",
    "\n",
    "n_inst = all_label.shape[0] #total number of images\n",
    "np.random.seed(22)\n",
    "indices = np.random.permutation(n_inst) #\n",
    "n_train = int(0.8 * n_inst)\n",
    "training_idx, test_idx = indices[:n_train], indices[n_train:]\n",
    "\n",
    "\n",
    "    \n",
    "label, test_label = all_label[training_idx,:], all_label[test_idx,:]\n",
    "\n",
    "  \n",
    "    \n",
    "with h5py.File('label.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"label\",  data=label)    \n",
    "\n",
    "    \n",
    "with h5py.File('test_label.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"test_label\",  data=test_label)    \n",
    "del all_label   \n",
    "del label\n",
    "del test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = h5py.File('data.h5', 'r')\n",
    "data = np.array(f.get('data'))[:, :, :]\n",
    "f.close()\n",
    "\n",
    "data_patches=[]\n",
    "for img in data: \n",
    "    patches=image.extract_patches_2d(img, patch_size=(64,64), max_patches=16,   random_state=0)\n",
    "    data_patches.append(patches)\n",
    "    \n",
    "data_patches=np.vstack(data_patches)\n",
    "print(data_patches.shape)\n",
    "with h5py.File('data_patches.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"data_patches\",  data=data_patches)    \n",
    "\n",
    "    \n",
    "del data_patches\n",
    "\n",
    "f = h5py.File('test_data.h5', 'r')\n",
    "test_data = np.array(f.get('test_data'))[:, :, :]\n",
    "f.close()\n",
    "\n",
    "test_data_patches=[]\n",
    "for img in test_data: \n",
    "    patches=image.extract_patches_2d(img, patch_size=(64,64), max_patches=16,   random_state=0)\n",
    "    test_data_patches.append(patches)\n",
    "\n",
    "test_data_patches=np.vstack(test_data_patches)\n",
    "print(test_data_patches.shape)\n",
    "with h5py.File('test_data_patches.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"test_data_patches\",  data=test_data_patches)  \n",
    "\n",
    "del data\n",
    "del test_data\n",
    "del data_patches\n",
    "del test_data_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('label.h5', 'r')\n",
    "label = np.array(f.get('label'))[:, :, :]\n",
    "f.close()\n",
    "\n",
    "label_patches=[]\n",
    "for img in label: \n",
    "    patches=image.extract_patches_2d(img, patch_size=(64,64), max_patches=16,   random_state=0)\n",
    "    label_patches.append(patches)\n",
    "    \n",
    "label_patches=np.vstack(label_patches)\n",
    "print(label_patches.shape)\n",
    "with h5py.File('label_patches.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"label_patches\",  data=label_patches)    \n",
    "\n",
    "    \n",
    "del label_patches\n",
    "\n",
    "f = h5py.File('test_label.h5', 'r')\n",
    "test_label = np.array(f.get('test_label'))[:, :, :]\n",
    "f.close()\n",
    "\n",
    "test_label_patches=[]\n",
    "for img in test_label: \n",
    "    patches=image.extract_patches_2d(img, patch_size=(64,64), max_patches=16,   random_state=0)\n",
    "    test_label_patches.append(patches)\n",
    "\n",
    "test_label_patches=np.vstack(test_label_patches)\n",
    "print(test_label_patches.shape)\n",
    "with h5py.File('test_label_patches.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"test_label_patches\",  data=test_label_patches)  \n",
    "del label\n",
    "del test_label\n",
    "del label_patches\n",
    "del test_label_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
